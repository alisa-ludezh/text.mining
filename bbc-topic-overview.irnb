{"cells":[{"metadata":{"_uuid":"14727e3089a3a45c00b3851f6ad1a98f1e17f71d","_execution_state":"idle","trusted":true,"scrolled":true},"cell_type":"code","source":"## Importing packages\n\n# This R environment comes with all of CRAN and many other helpful packages preinstalled.\n# You can see which packages are installed by checking out the kaggle/rstats docker image: \n# https://github.com/kaggle/docker-rstats\n\nlibrary(tidyverse) # metapackage with lots of helpful functions\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(scales)\nlibrary(forcats)\nlibrary(wordcloud)\ndevtools::install_github(\"cpsievert/LDAvisData\")\nlibrary(LDAvis)\n## Running code\n\n# In a notebook, you can run a single code cell by clicking in the cell and then hitting \n# the blue arrow to the left, or by clicking in the cell and pressing Shift+Enter. In a script, \n# you can run code by highlighting the code you want to run and then clicking the blue arrow\n# at the bottom of this window.\n\n## Reading in files\n\n# You can access files from datasets you've added to this kernel in the \"../input/\" directory.\n# You can see the files added to this kernel by running the code below. \n\nfolder = \"../input/bbc news summary/BBC News Summary/\"\n\n## Saving data\n\n# If you save any files or images, these will be put in the \"output\" directory. You \n# can see the output directory by committing and running your kernel (using the \n# Commit & Run button) and then checking out the compiled version of your kernel.\n\ntxt_from_folder <- function(subfolder) {\n    path = str_c(folder, subfolder)\n    print(path)\n    files <- list.files(path, pattern = \"txt\") #список файлов в папке\n    \n    for (i in (1:length(files))) {\n        filename <- str_c(path, files[i])\n        lines <- read_lines(filename,\n                               locale = locale(encoding = \"UTF-8\"))\n        lines <- lines[lines != '']\n        text <- data_frame(doc_name = str_c(str_replace(files[i],\".txt\",\"\"), subfolder), \n                             line = 1:length(lines), text = lines, theme = subfolder)\n\n        if (i==1) {texts <- text}\n        else {texts <- union_all(texts, text)}\n     }\n\n    texts\n}\n\n\ndocs_politics <- txt_from_folder(\"News Articles/politics/\")\ndocs_business <- txt_from_folder(\"News Articles/business/\")\n\nhead(docs_politics)\n\nfull_df <- union_all(docs_business, docs_politics)\n\nfull_df <- full_df %>%\n    mutate(text = str_to_lower(text))\n\narticles <- full_df %>%\n  unnest_tokens(word, text)\n\nhead(articles)\n\narticles %>%\n  count(word, sort = TRUE) %>%\n  top_n(10, wt = n) %>%\n  ungroup()\n\nВидим, что в первом десятке часто встречающихся слов - только слова общего назначения.\nУдалим их, используя список stop_word из пакета tidytext\n\ntidy_articles <- articles %>%\n    mutate(word = str_extract(word, \"[a-z']+\")) %>%\n    anti_join(stop_words) %>%\n    filter(str_detect(word, \"[a-z']\"))\n\ntidy_articles %>%\n  group_by(theme) %>%\n  count(word, sort = TRUE) %>%\n  top_n(10, wt = n) %>%\n  ungroup() %>%\n  ggplot(aes(x = reorder(word, n), y = n, fill = theme)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +\n  labs(title = \"Наиболее часто встречающиеся слова в статьях\",\n       x = \"Слова\", y = \"Количество слов в документе\",\n       fill = \"Статьи\")\n\n\n\ntidy_articles %>%\n    count(word) %>%\n    with(wordcloud(word, n, max.words = 100))\n\n\nfrequency <- tidy_articles %>%\n  count(theme, word) %>%\n  group_by(theme) %>%\n  mutate(proportion = n / sum(n)) %>% \n  select(-n) %>% \n  spread(theme, proportion, fill = 0)\nhead(frequency)\n\ncor.test(data = frequency, \n         ~ `News Articles/business/` + `News Articles/politics/`)\n\n\n\narticle_words <- full_df %>%\n  unnest_tokens(word, text) %>%\n  mutate(word = str_extract(word, \"[a-z']+\")) %>%\n  filter(!is.na(word))\n\nword_freq <- article_words%>%\n  count(theme, word) %>%\n  group_by(theme) %>%\n  mutate(freq = n / sum(n)) \n\nword_freq2 <- tidy_articles %>%\n    mutate(theme = str_c(theme,  \" without stop words\")) %>%\n  count(theme, word) %>%\n  group_by(theme) %>%\n  mutate(freq = n / sum(n))\n    \n\n\nunion_all(word_freq, word_freq2) %>% \n  ggplot(aes(freq, fill = theme)) +\n  geom_histogram(show.legend = FALSE, bins = 40) +\n  #xlim(NA, 0.006) +\n  facet_wrap(~theme, ncol = 2) +\n  labs(title = \"Распределение частот слов в статьях\",\n       y = \"Количество слов\", x = \"Частота слова\")\n\nword_tf_idf <- word_freq %>%\n  bind_tf_idf(word, theme, n) %>%\n  select(-freq) \n\nhead(word_tf_idf)\n\nword_tf_idf %>%\n  arrange(desc(tf_idf)) %>%\n  top_n(20) %>%\n  ggplot(aes(x = reorder(word, tf_idf), tf_idf, fill = theme)) +\n  geom_col() +\n  labs(x = NULL, y = \"tf-idf\", fill = \"Статьи\", title = \"Наиболее важные слова в статьях\") +\n  coord_flip()\n\nСформируем список из всех пяти тем документов:\n\nnew_full_df <- union_all(full_df,\ntxt_from_folder(\"News Articles/tech/\"),\ntxt_from_folder(\"News Articles/entertainment/\"),\ntxt_from_folder(\"News Articles/sport/\"))\n\ntail(new_full_df)\n\nСформируем свой кастомный список стоп-слов:\n\nwords_tf_idf <- new_full_df %>%\n  unnest_tokens(word, text) %>%\n  mutate(word = str_extract(word, \"[a-z']+\")) %>%\n  filter(!is.na(word)) %>%\n  count(theme, word) %>%\n  bind_tf_idf(word, theme, n) %>%\n  group_by(theme) %>%\n  mutate(total = sum(n)) %>%\n  ungroup() %>%\n  arrange(desc(tf_idf))\n\ncustom_stop_words <- words_tf_idf %>% \n  filter(idf==0) %>%\n  group_by(word) %>%\n  summarise(frequency = sum(n)/sum(total)) %>%\n  filter(frequency > 0.00015) %>%\n  arrange(desc(frequency))\n\nprint(custom_stop_words)\n\ntidy_articles <- new_full_df %>%\n    unnest_tokens(word, text) %>%\n    filter(!word %in% custom_stop_words$word) %>%\n    filter(!word %in% stop_words$word) %>%\n    filter(str_detect(word,\"[a-z]\"))\n\n\n# compute the table of terms:\nterm.table <- table(unlist(tidy_articles$word))\nterm.table <- sort(term.table, decreasing = TRUE)\n\n# remove terms that are stop words or occur fewer than 5 times:\ndel <- names(term.table) %in% stop_words | term.table < 5\nterm.table <- term.table[!del]\nvocab <- names(term.table)\n\n# now put the documents into the format required by the lda package:\nget.terms <- function(x) {\n  index <- match(x$word, vocab)\n  index <- index[!is.na(index)]\n  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))\n}\ndocuments <- lapply(split.data.frame(tidy_articles, tidy_articles$doc_name), get.terms)\n\n# Compute some statistics related to the data set:\nD <- length(documents)  # number of documents (2,000)\nW <- length(vocab)  # number of terms in the vocab (14,568)\ndoc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]\nN <- sum(doc.length)  # total number of tokens in the data (546,827)\nterm.frequency <- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]\nprint(c(D, W, N))\n\n# MCMC and model tuning parameters:\nK <- 5\nG <- 1000\nalpha <- 0.02\neta <- 0.02\n\n# Fit the model:\nlibrary(lda)\nset.seed(357)\nt1 <- Sys.time()\nfit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab, \n                                   num.iterations = G, alpha = alpha, \n                                   eta = eta, initial = NULL, burnin = 0,\n                                   compute.log.likelihood = TRUE)\nt2 <- Sys.time()\nt2 - t1  # about 24 minutes on laptop\n\ntheta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))\nphi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))\n\nlda_data <- list(phi = phi,\n                     theta = theta,\n                     doc.length = doc.length,\n                     vocab = vocab,\n                     term.frequency = term.frequency)\n\n\n# create the JSON object to feed the visualization:\njson2 <- createJSON(phi = lda_data$phi, \n                   theta = lda_data$theta, \n                   doc.length = lda_data$doc.length, \n                   vocab = lda_data$vocab, \n                   term.frequency = lda_data$term.frequency)\n\nserVis(json2, out.dir = \"./\", open.browser = F)\n\nsystem(\"mv index.html results.html\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99efcab549e462805371cbc903271615bfe20ee7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}